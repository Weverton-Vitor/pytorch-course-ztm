{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNGeNLWsbDSv1bJtnufvyOv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 06. PyTorch Transform Learning  \n","\n","What is transfer learning?\n","\n","Transfer learning involves taking the parameters of what one model has learned on another dataset and  applying to our own problem.\n","\n","* Pretreined models = foundation models"],"metadata":{"id":"iqbSqe4TZmyk"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","\n","print(torch.__version__)\n","print(torchvision.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2sLd4O9ajCN","executionInfo":{"status":"ok","timestamp":1701125143536,"user_tz":180,"elapsed":12047,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"003f003f-ad4c-431f-fea7-835b4640bf25"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu118\n","0.16.0+cu118\n"]}]},{"cell_type":"markdown","source":["Let's import the code we've written in previous sections so that we don't have  to write all again"],"metadata":{"id":"-n0gYNZVaxEl"}},{"cell_type":"code","source":["# Continue with regular imports\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# Try to import the going_modular directory, download it from GitHub if it doesn't work\n","try:\n","    from going_modular.going_modular import data_setup, engine\n","except:\n","    # Get the going_modular scripts\n","    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n","    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n","    !mv pytorch-deep-learning/going_modular .\n","    !rm -rf pytorch-deep-learning\n","    from going_modular.going_modular import data_setup, engine"],"metadata":{"id":"kZwnGX7bdQ-0","executionInfo":{"status":"ok","timestamp":1701125192434,"user_tz":180,"elapsed":48903,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2c0d06f-a893-4933-d630-3eb66b99c4b9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Couldn't find torchinfo... installing it.\n","[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n","Cloning into 'pytorch-deep-learning'...\n","remote: Enumerating objects: 4036, done.\u001b[K\n","remote: Counting objects: 100% (1224/1224), done.\u001b[K\n","remote: Compressing objects: 100% (225/225), done.\u001b[K\n","remote: Total 4036 (delta 1068), reused 1078 (delta 996), pack-reused 2812\u001b[K\n","Receiving objects: 100% (4036/4036), 651.02 MiB | 32.77 MiB/s, done.\n","Resolving deltas: 100% (2361/2361), done.\n","Updating files: 100% (248/248), done.\n"]}]},{"cell_type":"code","source":["# Setup device agnostic code\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wFD_W4P1e9DQ","executionInfo":{"status":"ok","timestamp":1701125192434,"user_tz":180,"elapsed":23,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"835feba0-8882-4ce7-8d8a-9e6ff7f4b712"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0OiU-03lfKZq","executionInfo":{"status":"ok","timestamp":1701125192434,"user_tz":180,"elapsed":15,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"0d6b53ee-e287-4376-98c7-1289f3456adc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"markdown","source":["## 1. Get data\n","\n","We need our pizza, steak, sushi data to build a transfer learning model on.\n"],"metadata":{"id":"tjmB5C30fMrH"}},{"cell_type":"code","source":["import os\n","import zipfile\n","from pathlib import Path\n","import requests\n","\n","\n","# Setup data path\n","data_path = Path('data/')\n","image_path = data_path / 'pizza_strak_sushi' # Images from a subset of classes from the Food101 dataset\n","\n","# If the image folder don't exist,  download it and prepare it...\n","if image_path.is_dir():\n","  print(f\"{image_path} directory exists, skipping re-download.\")\n","else:\n","  print(f\"Did not find {image_path}, donwloading it...\")\n","  image_path.mkdir(parents=True, exist_ok=True)\n","\n","  # Donwload pizza, steak, shushi data\n","  with open(data_path/'pizza_steak_sushi.zip', 'wb') as f:\n","    request = requests.get('https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip')\n","    print(\"Downloading pizza, steak, sushi data...\")\n","    f.write(request.content)\n","\n","  # Unzip pizza, steak, shushi data\n","  with zipfile.ZipFile(data_path/'pizza_steak_sushi.zip', 'r') as zip_ref:\n","    print(\"Unzipping pizza, steak, sushi data...\")\n","    zip_ref.extractall(image_path)\n","\n","  # Remove .zip file\n","  os.remove(data_path/'pizza_steak_sushi.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-XpnrIXgxy0","executionInfo":{"status":"ok","timestamp":1701125193721,"user_tz":180,"elapsed":1297,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"65deca9f-1fa7-42da-fd46-424ece98cc06"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Did not find data/pizza_strak_sushi, donwloading it...\n","Downloading pizza, steak, sushi data...\n","Unzipping pizza, steak, sushi data...\n"]}]},{"cell_type":"code","source":["# Setup directory path\n","train_dir = image_path/'train'\n","test_dir = image_path/'test'\n","\n","train_dir, test_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPnfWQuQhpAi","executionInfo":{"status":"ok","timestamp":1701125193721,"user_tz":180,"elapsed":12,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"2d8231cd-1e64-4671-c1d4-23b44951badc"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(PosixPath('data/pizza_strak_sushi/train'),\n"," PosixPath('data/pizza_strak_sushi/test'))"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## 2. Create Datasets and DataLoaders\n","\n","Now we've got some data, want to turn it into PyTorch DataLoaders\n","\n","To do so, we can use the `data_setup.py` and `create_dataloaders()` function we mande in 05. Pytorch Going Modular\n","\n","There's one thing we have to think about when loading: how to **transfom** it?\n","\n","And with `torchvision` 0.13+ there's two ways to do this:\n","\n","1. Manually created transforms - you define what transforms you want your data to go through.\n","2. Automatically created transforms - the transform for your data are defined by the model you'd like to use\n","\n","Important point: When using a pretrained model, it's import that the data(inclunding your custom data) that you pass through it is **transformed** in the same way that the data the model was trained on."],"metadata":{"id":"Ivojp7qIo_eT"}},{"cell_type":"markdown","source":["### 2.1 Creating a transform for `torchvision.model`(manual creation)\n","\n","`torchvision.models` contains pretreined models(models ready fo transfer learling) right within `torchvision`\n","\n","All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\n","\n","The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n","\n","You can use the following transform to normalize:\n","\n","- normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n"],"metadata":{"id":"rnxJaE9ZsVLT"}},{"cell_type":"code","source":["from torchvision import transforms\n","\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","manual_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)), # Resize image to 224x224, heigthxwidth\n","    transforms.ToTensor(), # get image into range of [0, 1] in tensor format\n","    normalize # make sure images have the same distribution as ImageNet(where our pretreined models had been trained)\n","])"],"metadata":{"id":"fiZiANx3uDkS","executionInfo":{"status":"ok","timestamp":1701125193721,"user_tz":180,"elapsed":10,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from going_modular.going_modular import data_setup\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n","                                                                               test_dir=test_dir,\n","                                                                               transform=manual_transforms,\n","                                                                               batch_size=32)"],"metadata":{"id":"vNuuO35pwGSs","executionInfo":{"status":"ok","timestamp":1701125193721,"user_tz":180,"elapsed":9,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_dataloader, test_dataloader, class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnQhb5YMw1zy","executionInfo":{"status":"ok","timestamp":1701125193722,"user_tz":180,"elapsed":10,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"9634fc89-566e-4871-a052-c36a9d601283"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x79981afa2f50>,\n"," <torch.utils.data.dataloader.DataLoader at 0x79981afa2ef0>,\n"," ['pizza', 'steak', 'sushi'])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Get a set of pretreinedmodel weights\n","weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # Default -> Best available weights\n","weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jgXaG_ow4l0","executionInfo":{"status":"ok","timestamp":1701125193722,"user_tz":180,"elapsed":8,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"646937e8-9c17-4382-8b0f-49169c97b735"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EfficientNet_B0_Weights.IMAGENET1K_V1"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Get the transforms used to create our pretreined weights\n","auto_transforms = weights.transforms()\n","auto_transforms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4XhejQNzeGx","executionInfo":{"status":"ok","timestamp":1701125193722,"user_tz":180,"elapsed":6,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"1133f449-2c0e-4ce1-a5a7-c854d1dee8b7"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ImageClassification(\n","    crop_size=[224]\n","    resize_size=[256]\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BICUBIC\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Create DataLoaders using the automatic transforms\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n","                                                                               test_dir=test_dir,\n","                                                                               transform=auto_transforms,\n","                                                                               batch_size=32)"],"metadata":{"id":"TAAKJwpxzt8_","executionInfo":{"status":"ok","timestamp":1701125194154,"user_tz":180,"elapsed":6,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_dataloader, test_dataloader, class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVvB_UTI0FcT","executionInfo":{"status":"ok","timestamp":1701125194154,"user_tz":180,"elapsed":5,"user":{"displayName":"Weverton Vitor","userId":"12691862709549236158"}},"outputId":"a7177385-3c67-4628-9b3d-5ff217e9c606"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x79981afa14b0>,\n"," <torch.utils.data.dataloader.DataLoader at 0x79981afa34f0>,\n"," ['pizza', 'steak', 'sushi'])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## 3. Getting a pretreined model\n","\n","There are various places to get a pretreined model, such as:\n","1. PyTorch domain libraries\n","2. Libraries like `timm` (Torch image models)\n","3. HugginFace Hub (For plenty of different models)\n","4. PapersWithCode (For models across diferent problem/domains)\n"],"metadata":{"id":"qnsK5hJO0Nqj"}},{"cell_type":"markdown","source":["### 3.1 Which pretreined model should you use?\n","\n","**Experiment, Experiment, Experiment**\n","\n","The whole idea of transfer learning: take an already well-perfoming model from a\n","problem space similar to you own and then customize to your own problem\n","\n","Three things to consider\n","\n","1. Speed - how fast does it run?\n","2. Size - how big is the model?\n","3. Performance - how well does it go own your chosen problem(e.g how well does it\n","classify food images? for FoodVision Mini?)\n","\n","Where does the model live?\n","\n","Is it on device (likea self-driving card)\n","\n","Or does it live on a server?\n","\n","Looking at: https://pytorch.org/vision/stable/models#table-of-all-available-classification-weights\n","\n","Which model should we choose?\n","\n","For our case(deploying FoodVision Mini on a mobile device), it looks like EffNet50\n","is one of our best options in terms perfomance vs size.\n","\n","However, in light of The Bitter Lesson, if we had infinite compute, we'd likely\n","pick the biggest model + most parameters + most general we could.\n","http://www.incompleteideas.net/IncIdeas/BitterLesson.html"],"metadata":{"id":"iaG9teRicokZ"}},{"cell_type":"markdown","source":["### 3.2 Setting up a pretreined model\n","\n","Want to create an instance of a pretreined EffnetB0 - https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights"],"metadata":{"id":"CnkGQirihwpD"}},{"cell_type":"code","source":["!pip install torchvision==0.15.2 # 0.16.0 has error to download models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W--DBsB5oVJr","outputId":"eea84107-4ef7-4e97-861f-ee09716d8ed2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchvision==0.15.2\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.31.0)\n","Collecting torch==2.0.1 (from torchvision==0.15.2)\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["print(f\"torch version: {torch.__version__}\")\n","print(f\"torchvision version: {torchvision.__version__}\")"],"metadata":{"id":"2FBchJG9n1bQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Old method of creating a pretreined model(prior to torchvision v0.13)\n","#model = torchvision.models.efficientnet_b0(pretrained=True)\n","\n","# New method of creating a pretreined model (torchvision v0.13)\n","weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # DEFTAULT -> Best available weights\n","model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n","\n","model"],"metadata":{"id":"T_hFP_2djHMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature extractor, also called of \"Backbone\"\n","model.features"],"metadata":{"id":"NNlo67e5jXOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compress features in a sigle vector\n","model.avgpool"],"metadata":{"id":"uQk2WePwpqXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.classifier"],"metadata":{"id":"MyX7UMwppu9A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 Getting a summary of our model with `torchinfo.summary()`\n"],"metadata":{"id":"LJ2A1ZsipxWo"}},{"cell_type":"code","source":["# Print with torchinfo\n","from torchinfo import summary\n","\n","summary(model=model,\n","        input_size=[1,3,224,224], # example of [batch_size, color_channels, height, width]\n","        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n","        col_width=20,\n","        row_settings=['var_names'])"],"metadata":{"id":"BAGO1iRv2LYH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.4 Freezing the base model and changing the output layer to suit our needs\n","\n","Freeze a layer means that layer's weights will not update during training\n","\n","With a feature extractor model, typically you will \"freeze\" the base layers of\n","a pretrained/foundation model and update the output layers to suit your own problem"],"metadata":{"id":"67LlFsMJ2RgY"}},{"cell_type":"code","source":["# Freeze all of the base layers in EffNet50\n","for param in model.features.parameters():\n","  # print(param)\n","  param.requires_grad = False"],"metadata":{"id":"OG7W8Iia5UnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print with torchinfo\n","from torchinfo import summary\n","\n","summary(model=model,\n","        input_size=[1,3,224,224], # example of [batch_size, color_channels, height, width]\n","        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n","        col_width=20,\n","        row_settings=['var_names'])"],"metadata":{"id":"Sdl-r4cC6K09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Update the classifier head of our model to suit our problem\n","from torch import nn\n","\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.2, inplace=True), # Bernouli distribution, 20% maybe 0 values\n","    nn.Linear(in_features=1280, # Feature vector coming in\n","              out_features=len(class_names)).to(device), # How many classes do we have?\n",")\n","\n","model.classifier"],"metadata":{"id":"G4bWvFUj6Q8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4 Train mode"],"metadata":{"id":"kl1W86Qq9iX2"}},{"cell_type":"code","source":["# Define loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"C3N2nttD-1G9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import train function\n","from going_modular.going_modular import engine\n","\n","# Set  the manual seed\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Setup training and save the results\n","results = engine.train(model=model,\n","                       train_dataloader=train_dataloader,\n","                       test_dataloader=test_dataloader,\n","                       optimizer=optimizer,\n","                       loss_fn=loss_fn,\n","                       epochs=5,\n","                       device=device)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"[INFO] Total training time: {end_time-start_time}\")"],"metadata":{"id":"Whl_DhnM_SsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results"],"metadata":{"id":"nE8snOB9_bxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results"],"metadata":{"id":"en7MT7EqnKmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Evaluate model by plot loss curves"],"metadata":{"id":"FmzsRCRBpLD3"}},{"cell_type":"code","source":["# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\n","try:\n","    from helper_functions import plot_loss_curves\n","except:\n","    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n","    with open(\"helper_functions.py\", \"wb\") as f:\n","        import requests\n","        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n","        f.write(request.content)\n","    from helper_functions import plot_loss_curves\n","\n","# Plot the loss curves of our model\n","plot_loss_curves(results)"],"metadata":{"id":"-5qXtJjupPR-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Make predictions on image from the test set\n","\n","Let's adhere to the data explorer's motto of *Visualize, Visualize, Visualize*!\n","\n","And make some qualitative predictions on our test set\n","\n","Some things to keep in mind when making predictions/inference on test data/custom\n","data.\n","\n","We have to make sure that our test/custom data is:\n","* Same shape - images need to be same shape\n","* Same datatype - custom data should be in the same data type\n","* Same device - custom data/test data should be on the same device\n","* Same transform - if you've transformed your train data, ideally you will\n","transform the test data and custom data the same\n","\n","To do all of this automagically, let's create a function called `pred_and_plot_image()`, that:\n","\n","1. Take in a trained model, a list of class names, a filepath to a target image,\n","an image size, a transform and a target device\n","2. Open the image with `PIL.Image.Open()`\n","3. Create a transform if one doesn't exist\n","4. Make sure the model is on the target device\n","5. Turn the model to `model.eval()` mode to make sure it's ready for inference\n","(this will turn off things like `nn.Dropout()`)\n","6. Transform the target image and make sure it's dimesionality is suited for the model (this mainly relates to batch size)\n","7. Make a prediction on the image by passing to the model\n","8. Convert the model's output logits to prediction probabilities using `torch.softmax()`\n","9. Convert model's probabilities to prediciton labels using `torch.argmax()`\n","10.Plot the image with `matplotlib` and set  the title to prediction label from step 9 and prediction probabilitie from step 8"],"metadata":{"id":"afOMePIeplZn"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from typing import List, Tuple\n","from PIL import Image\n","from torchvision import transforms\n","\n","# 1. Take in params\n","def pred_and_plot_image(model: nn.Module,\n","                        image_path: str,\n","                        class_names:List[str],\n","                        image_size: Tuple[int, int]=(224, 224),\n","                        transform:transforms.Compose|None=None,\n","                        device=device):\n","  # 2. Open image with pil\n","  image = Image.open(image_path)\n","\n","  # 3. Create a transform if one doesn't exist and transform image\n","  if transform is None:\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    transform = transforms.Compose([\n","        transforms.Resize(image_size), # Resize image to 224x224, heigthxwidth\n","        transforms.ToTensor(),\n","        normalize\n","    ])\n","\n","  # 4. Send model to target device\n","  model.to(device)\n","\n","  # 5. Eval mode\n","  model.eval()\n","\n","  # 6. Transform the image and add batch dimension\n","  transformed_image = transform(image)\n","  transformed_image = transformed_image.unsqueeze(dim=0)\n","\n","  # 7. Pass image through the model\n","  with torch.inference_mode():\n","    y_logits = model(transformed_image.to(device))\n","\n","    # 8. Raw logits to prediction probabilities\n","    y_pred = torch.softmax(y_logits, dim=1)\n","\n","    # 9. Prediction probabilities to prediction labels\n","    label_idx = int(torch.argmax(y_pred, dim=1).item())\n","\n","    # 10. Plot image\n","    plt.figure(figsize=(10,7))\n","    plt.imshow(image)\n","    plt.title(f\"{class_names[label_idx]} - {y_pred[0][label_idx]:.4f}%\")\n","    plt.axis(False)\n"],"metadata":{"id":"PQh3BEppsoYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Get a random list of paths from the\n","import random\n","\n","num_image_to_plot = 3\n","test_image_path_list =  list(Path(test_dir).glob('*/*.jpg'))\n","#random_samples_idx = random.sample(range(0, len(test_image_path_list)), k=num_image_to_plot)\n","random_samples_idx = random.sample(population=test_image_path_list, k=num_image_to_plot)\n","for sample_path in random_samples_idx:\n","  pred_and_plot_image(model=model,\n","                      image_path=str(sample_path),\n","                      class_names=class_names,\n","                      image_size=(224, 224),\n","                      device=device)"],"metadata":{"id":"18rBAHWf3FNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.1  Making predictions on a custom image"],"metadata":{"id":"8FiVpJkeP_wH"}},{"cell_type":"code","source":["pred_and_plot_image(model=model,\n","                    image_path='./test.jpg',\n","                    class_names=class_names,\n","                    device=device,\n","                    image_size=(224, 224))"],"metadata":{"id":"wZEJBKsb2mYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download custom image\n","import requests\n","\n","# Setup custom image path\n","custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n","\n","# Download the image if it doesn't already exist\n","if not custom_image_path.is_file():\n","    with open(custom_image_path, \"wb\") as f:\n","        # When downloading from GitHub, need to use the \"raw\" file link\n","        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n","        print(f\"Downloading {custom_image_path}...\")\n","        f.write(request.content)\n","else:\n","    print(f\"{custom_image_path} already exists, skipping download.\")\n","\n","# Predict on custom image\n","pred_and_plot_image(model=model,\n","                    image_path=custom_image_path,\n","                    class_names=class_names)"],"metadata":{"id":"NV86jD3nLv9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rgahd-kMQjpk"},"execution_count":null,"outputs":[]}]}